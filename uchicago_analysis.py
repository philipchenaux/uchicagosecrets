# -*- coding: utf-8 -*-
"""secrets.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Z8Ehwafmbu8F_RllNdd4p6Y-iaY3ItS9
"""

import os

#Web scraping libraries
import csv
from bs4 import BeautifulSoup
import re
!pip install selenium
from selenium import webdriver
import time

!apt-get update # to update ubuntu to correctly run apt install
!apt install chromium-chromedriver
!cp /usr/lib/chromium-browser/chromedriver /usr/bin
import sys

# Imports the Google Cloud client library
from google.cloud import language
from google.cloud.language import enums
from google.cloud.language import types

#Data manupulation libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

#Libraries to manipulate dates
!pip install datetime
import datetime

#Date and dependencies are in a google drive folder
#Connect google drive to this notebook
from google.colab import drive
drive.mount('/content/gdrive')

#ensure the file is accessible
#Make sure you have this exact path
!ls /content/gdrive/'My Drive'/'Colab Notebooks'/'secrets hackathon'/dependencies

os.environ["GOOGLE_APPLICATION_CREDENTIALS"]="/content/gdrive/My Drive/Colab Notebooks/secrets hackathon/dependencies/creds.json"

#ensure the path is set correctly
!echo $GOOGLE_APPLICATION_CREDENTIALS

#print token
!gcloud auth application-default print-access-token

#unzipping our google cloud sdk file
!tar -xvf "/content/gdrive/My Drive/Colab Notebooks/secrets hackathon/dependencies/google-cloud-sdk-280.0.0-linux-x86_64.tar.gz" 

#installing google cloud language
!pip install --upgrade google-cloud-language

#Scrapes Facebook for UChicago secret posts. Get both post content and date-time
#This will take a while to run

sys.path.insert(0,'/usr/lib/chromium-browser/chromedriver')

chrome_options = webdriver.ChromeOptions()
chrome_options.add_argument('--headless')
chrome_options.add_argument('--no-sandbox')
chrome_options.add_argument('--disable-dev-shm-usage')
browser = webdriver.Chrome('chromedriver',chrome_options=chrome_options)
browser.get('https://www.facebook.com/pg/secretsuchicago/posts/?ref=page_internal/')
SCROLL_PAUSE_TIME = 3

# Get scroll height
last_height = browser.execute_script("return document.body.scrollHeight")
while True:
    #Scroll down to bottom
    browser.execute_script("window.scrollTo(0, document.body.scrollHeight);")
    #Wait to load page
    time.sleep(SCROLL_PAUSE_TIME)
    #Calculate new scroll height and compare with last scroll height
    new_height = browser.execute_script("return document.body.scrollHeight")
    if new_height == last_height:
        break
    last_height = new_height
html = browser.page_source

soup = BeautifulSoup(html,'lxml')

file = open('uchicagosecrets.csv','w',encoding='utf-8')
writer = csv.writer(file)

writer.writerow(['PostText','Date'])

for userContentWrapper in soup.find_all('div',class_='userContentWrapper'):
    content = userContentWrapper.find(class_='userContent').find('p').findNext('p').text
    date = userContentWrapper.find('abbr')['title']
    writer.writerow([content,date])

file.close()
browser.close()

#The start dates for given quarters
spring_18 = datetime.date(2018, 3, 26)
autumn_18 = datetime.date(2018, 10, 1)
winter_19 = datetime.date(2019, 1, 7)
spring_19 = datetime.date(2019, 4, 1)
autumn_19 = datetime.date(2019, 10, 1)
winter_20 = datetime.date(2020, 1, 6)
spring_20 = datetime.date(2020, 3, 30)

#Convert our strings to categorical date-data
#Determine in which quarter a given post date was written in by determining
# if it happens before the next quarter's start date

def date_to_data(string):
  post_date = datetime.datetime.strptime(string, '%m/%d/%y, %I:%M %p')
  post_date = post_date.date()
  if (post_date < autumn_18):
    return 'Spring 18'
  elif (post_date < winter_19):
    return 'Autumn 18'
  elif (post_date < spring_19):
    return 'Winter 19'
  elif (post_date < autumn_19):
    return 'Spring 19'
  elif (post_date < winter_20):
    return 'Autumn 19'
  elif (post_date < spring_20):
    return 'Winter 20'
  else:
    return 'Invalid Date'

#reading in our data
df = pd.read_csv (r'/content/uchicagosecrets.csv')
posts = df["PostText"]
dates = df["Date"]

#Make new dataframe
graph_data = pd.DataFrame(np.zeros((len(df.index), 2)))

for i in range(len(posts)):
  
  # Instantiates a client
  client = language.LanguageServiceClient()

  # The text to analyze
  text = posts[i]
  document = types.Document(
      content=text,
      type=enums.Document.Type.PLAIN_TEXT)

  # Detects the sentiment of the text
  sentiment = client.analyze_sentiment(document=document).document_sentiment

  print('Text: {}'.format(text))
  print('Sentiment: {}, {}'.format(sentiment.score, sentiment.magnitude))

  value = (sentiment.score*sentiment.magnitude*10)
  post_date = date_to_data(dates[i])
  graph_data.loc[i] = pd.Series({0:value, 1:post_date})

final_graph =  pd.DataFrame(np.zeros((6, 2)))

winter_20 = graph_data.loc[graph_data[1] == 'Winter 20']
autumn_19 = graph_data.loc[graph_data[1] == 'Autumn 19']
spring_19 = graph_data.loc[graph_data[1] == 'Spring 19']
winter_19 = graph_data.loc[graph_data[1] == 'Winter 19']
autumn_18 = graph_data.loc[graph_data[1] == 'Autumn 18']
spring_18 = graph_data.loc[graph_data[1] == 'Spring 18']

final_graph.loc[5] = pd.Series({0:'Winter 20', 1:winter_20[[0]].mean(axis=0)})
final_graph.loc[4] = pd.Series({0: 'Autumn 19', 1:autumn_19[[0]].mean(axis=0)})
final_graph.loc[3] = pd.Series({0:'Spring 19', 1:spring_19[[0]].mean(axis=0)})
final_graph.loc[2] = pd.Series({0:'Winter 19', 1:winter_19[[0]].mean(axis=0)})
final_graph.loc[1] = pd.Series({0:'Autumn 18', 1:autumn_18[[0]].mean(axis=0)})
final_graph.loc[0] = pd.Series({0:'Spring 18', 1:spring_18[[0]].mean(axis=0)})

#plotting our results
plt.scatter(final_graph[0], final_graph[1])
plt.plot(final_graph[0], final_graph[1])
plt.show()

